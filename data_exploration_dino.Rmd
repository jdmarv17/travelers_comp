---
title: "data_exploration_dino"
author: "Dino"
date: "11/1/2021"
output: html_document
---


```{r}
library("ggplot2")
library("GGally")
library("readr")
library("tidyverse")
library("randomForest")
library("lubridate")
library("caret")
```

```{r data-import}
col_types_train = "iif?idllff?ffidiflddfdfdf"
col_types_test = "iif?idllff?ffidiflddfdfd"
train_total = read_csv("data/train_2021.csv", col_types = col_types_train)

```

```{r cleaning-imputation-and-calc}
# Fixing some date and logical columns
train_clean = train_total %>% mutate(claim_date = mdy(claim_date),
                                     witness_present_ind = as.logical(witness_present_ind),
                                     marital_status = as.logical(marital_status),
                                     fraud = factor(fraud))
# What to do with NAs?
# * NAs are present in the test set. 
# Either * NAs need to be imputed or
#        * the classification methods needs to deal with missing values

# In the training set:      
#   column                  # of NA     Prevalence
# * marital_status          5           TRUE
# * witness_present_ind     132         FALSE
# * claim_est_payout        17          4668.8
# * age_of_vehicle          8           5

train_clean %>% summary()

impute = function(x, method = median) {
  if (is.numeric(x)) {
    return(ifelse(!is.na(x)&x!=-1, x, method(x, na.rm = TRUE)))
  } else if (is.factor(x)) {
    x[is.na(x)] = names(which.max(table(x)))
    return(x)
  } else if (is.logical(x)) {
    ifelse(!is.na(x), x, as.logical(names(which.max(table(x)))))
  }
}

train_clean = train_clean %>% mutate_all(impute)

train_clean %>% summary()

# Some columns are calculated based on intuitions about the data:
train_clean = train_clean %>% mutate(payout_car = claim_est_payout / vehicle_price,
                                     payout_income = claim_est_payout / annual_income,
                                     vehicle_price_age = vehicle_price / (age_of_vehicle+1))
```


```{r}
# Bedford's law?

leading_digits = function(x, n) {
  as.numeric(substring(as.character(x), 1, n))
}

train_bf = train_clean %>% mutate(bf_income_1 = leading_digits(annual_income, 1),
                                  bf_income_2 = leading_digits(annual_income, 2),
                                  bf_payout = leading_digits(claim_est_payout, 1),
                                  bf_weight = leading_digits(vehicle_weight, 1))

histogram(train_bf$bf_income_1)
histogram(train_bf$bf_income_2)
histogram(train_bf$bf_payout)
histogram(train_bf$bf_weight)

train_bf %>% filter(bf_income_1 == 5) %>% summary() # highly skewed, maybe take 1st 2 digits?
train_bf %>% filter(bf_payout > 7) %>% summary() # somewhat interesting
train_bf %>% filter(bf_weight >= 6) %>% summary() # probably boring

```



```{r}
test_total = read_csv("data/test_2021.csv", col_types = col_types_test)

cols_of_interest =
  c("age_of_driver",    # positive integer
    "safty_rating",     # 0 - 100 score
    "annual_income",    # positive double
    "liab_prct",        # 0 - 100 score
    "claim_est_payout", # positive double
    "age_of_vehicle",   # positive integer
    "vehicle_price"     # positive double
    )

train_total %>% select(cols_of_interest) %>% na.omit() %>% cor()

```


```{r}
train_no_na = train_bf %>% select(-c("zip_code")) %>% na.omit()

set.seed(51)
train_sample = sample_n(train_no_na, 
                        5000, 
                        replace = FALSE, 
                        weight = if_else(train_no_na$fraud==1, 0.9, 0.1)
                        )
rf = randomForest(fraud~., data = train_sample, ntree = 500)
print(rf)

pred1 = predict(rf, train_no_na)
confusionMatrix(pred1, train_no_na$fraud)
```

```{r}

pred2 = predict(rf, test_total)

```

